{"cells":[{"cell_type":"code","source":["#import findspark\n#findspark.init('/home/nahle/spark-2.1.0-bin-hadoop2.7')\n#import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"NN\").getOrCreate()"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":["## Getting the data"],"metadata":{}},{"cell_type":"code","source":["data = sc.textFile('/FileStore/tables/SMSSpamCollection')\nmydata=data.map(lambda x: x.split('\\t')).map(lambda y: (y[0], y[1]))\ndata=mydata.toDF(['label','message'])"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["### We transform the input ham/spam in 0/1 class with StringIndexer"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder, StringIndexer\nfrom pyspark.ml.feature import Tokenizer, RegexTokenizer\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.ml.feature import StopWordsRemover\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import CountVectorizer\nstringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"categoryIndex\")\nstring_indexor = stringIndexer.fit(data)\ndata = string_indexor.transform(data)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["regex_tokenizer = RegexTokenizer(inputCol='message', outputCol='words', pattern='\\\\W') #Separate \nregex_df = regex_tokenizer.transform(data)\nfinal_data=regex_df.select(\"categoryIndex\",\"words\").withColumnRenamed(\"categoryIndex\",\"label\")\ncount_words = udf(lambda words: len(words), IntegerType())\nregex_tokenized_counts = regex_df.withColumn('freq', count_words('words'))\nregex_tokenized_counts.show()\nfinal_data.show(45,truncate=False)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### We filter some words with StopWordsRemover"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import StopWordsRemover\nremover = StopWordsRemover(inputCol='words', outputCol='tokens')\ntokens_filtered = remover.transform(regex_tokenized_counts)\ncleanDF= tokens_filtered.withColumn('count_tokens', count_words('tokens'))\ncount_vec = CountVectorizer(inputCol='tokens', outputCol='features',  minDF=1)\nmodel = count_vec.fit(cleanDF)\ndata = model.transform(cleanDF)\ncleanDF.show(truncate=True)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["###Rename columns"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *\ndata.withColumnRenamed(\"label\", \"type\").withColumnRenamed(\"categoryIndex\", \"label\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["#Logistic regression"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics\nfrom pyspark.mllib.evaluation import RegressionMetrics\n\nlr = LogisticRegression(featuresCol='features', labelCol='label')\nmodelData= data.selectExpr('features', 'categoryIndex as label') "],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["trainData, testData = modelData.randomSplit([0.7, 0.3])\nlrModel = lr.fit(trainData) #Fit the model\ntrainResults=lrModel.evaluate(trainData)  #evaluation on the training set\n\nsummary = lrModel.summary  # MODEL summary for lrModel\ntrain_res=summary.predictions.select(\"label\",\"prediction\")\ntrain_res.show(n=5)  # first predictions\n\ntrain_rdd=train_res.rdd\nmetrics = BinaryClassificationMetrics(train_rdd)\nmetrics2 = RegressionMetrics(train_rdd)\nprint(\"R2 score ={}:\" .format(metrics2.r2)) #R2 score\n"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["R2 score of 100% which indicates that the model explains all the variability of the response data around its mean"],"metadata":{}},{"cell_type":"code","source":["testResults = lrModel.evaluate(testData) #Use the model on our test data\ntest_res=testResults.predictions.select(\"label\",\"prediction\")\ntest_res.show(10) #Show difference between prediction and reality\n"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["test_rdd=test_res.rdd\nmetricss = BinaryClassificationMetrics(test_rdd)\nmetricss2 = RegressionMetrics(test_rdd)\nprint(metricss2.r2) "],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["R2 score is lower than before but it's because it was 100% on our train set, so maybe there is a little bit an overfitting."],"metadata":{}},{"cell_type":"markdown","source":["#Neural Network"],"metadata":{}},{"cell_type":"code","source":["#Training a Neural Network\nfrom pyspark.ml.classification import MultilayerPerceptronClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nlayers = [8623, 5, 4, 2]\nclassifier = MultilayerPerceptronClassifier(maxIter=100,\n                                           layers= layers,\n                                           blockSize = 128)\nmodel = classifier.fit(trainData)\npreds = model.transform(testData)\npreds.show(20)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["predictionsAndLabels = preds.select(['prediction', 'label'])\npredictionsAndLabels.show()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["#Classification metrics\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nevaluator_NN = MulticlassClassificationEvaluator(metricName='f1')\nevaluator_NN.explainParam('metricName')\nevaluation_NN = evaluator_NN.evaluate(predictionsAndLabels)\nprint ('NeuralNetwork F1 = %g'%evaluation_NN)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["#Naive Bayes"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nnb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\nmodel_Naive = nb.fit(trainData)\n\npredictions_Naive = model_Naive.transform(testData)\npredictions_Naive.show(5)\n\nevaluator_Naive = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n                                              metricName=\"accuracy\")\n\naccuracy = evaluator_Naive.evaluate(predictions_Naive)\nprint(\"Test set accuracy = \" + str(accuracy))"],"metadata":{},"outputs":[],"execution_count":23}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.5.2","nbconvert_exporter":"python","file_extension":".py"},"name":"Ham_And_Spam","notebookId":1006692025469327},"nbformat":4,"nbformat_minor":0}
