{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession"],"metadata":{"collapsed":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":["spark = SparkSession.builder.appName(\"ML\").getOrCreate()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["## Attributes on original data\n\n\n- AT = Atmospheric Temperature in C\n- V = Exhaust Vaccum Speed\n- AP = Atmospheric Pressure\n- RH = Relative Humidity\n- PE = Power Output"],"metadata":{}},{"cell_type":"markdown","source":["## Getting the data"],"metadata":{}},{"cell_type":"code","source":["originData = spark.read.csv(\"/FileStore/tables/powerDataForTP.csv\", inferSchema=True, header = True,sep=\";\")\noriginData.show()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["There are 9570 lines on the whole dataset but there are only 4519 rows with non null values. We will see what model what model is the best if we use only these 4519 rows or if we fill the null values by the mans of the column."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import mean\nmeanAT = originData.select(mean(originData['AT']))\nmeanV = originData.select(mean(originData['V']))\nmeanAP = originData.select(mean(originData['AP']))\nmeanRH = originData.select(mean(originData['RH']))\nmeanPE = originData.select(mean(originData['PE']))\n\navgAT = meanAT.collect()[0][0]\navgV = meanV.collect()[0][0]\navgAP = meanAP.collect()[0][0]\navgRH = meanRH.collect()[0][0]\navgPE = meanPE.collect()[0][0]\n\nrowData_avg = originData.na.fill(avgAT, subset=['AT'])\nrowData_avg = rowData_avg.na.fill(avgV, subset=['V'])\nrowData_avg = rowData_avg.na.fill(avgAP, subset=['AP'])\nrowData_avg = rowData_avg.na.fill(avgRH, subset=['RH'])\nrowData_avg = rowData_avg.na.fill(avgPE, subset=['PE'])\n\nrowData_avg.show()\n\nrowData=originData.na.drop() # We delete all rows containing null values\n\nprint(\"There are \", rowData.count(), \" lines with non null values among \" ,originData.count(), \" lines on the whole dataset\")\nrowData.describe().show()\nrowData_avg.describe().show()\noriginData.describe().show()\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["rowData.printSchema()\nrowData.columns"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["from pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.feature import VectorAssembler\nvec = VectorAssembler(\n  inputCols= [\n    'AT',\n    'V',\n    'AP',\n    'RH'\n    ],\n   outputCol = 'features'                  \n )"],"metadata":{"collapsed":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":["data = vec.transform(rowData)\ndata_avg=vec.transform(rowData_avg)\ndata.show(truncate=False)\ndata_avg.show(truncate=False)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":["modelData = data.selectExpr('features', 'PE as label') \n\n\nmodelData_avg = data_avg.selectExpr('features', 'PE as label')"],"metadata":{"collapsed":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":["modelData.show(5) \nmodelData_avg.show(5) "],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Split the data : 70% for training set, 30% for test set. We do that for both modelData and modelData_avg"],"metadata":{}},{"cell_type":"code","source":["trainData, testData = modelData.randomSplit([0.7, 0.3])\ntrainData_avg, testData_avg = modelData_avg.randomSplit([0.7, 0.3])"],"metadata":{"collapsed":true},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["#Linear regression"],"metadata":{}},{"cell_type":"code","source":["lr = LinearRegression(featuresCol='features', labelCol='label')"],"metadata":{"collapsed":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":["lrModel = lr.fit(trainData) #Linear regression model for data with null values dropped\nlrModel_avg = lr.fit(trainData_avg) #Linear regression model for data with null values replace by means\n\nsummary = lrModel.summary  # MODEL summary for lrModel\nsummary_avg = lrModel_avg.summary  # MODEL summary for lrModel_avg\n\nsummary.predictions.show(n=3, truncate = False)  #Three first predictions\nsummary_avg.predictions.show(n=3, truncate = False)  #Three first predictions"],"metadata":{"collapsed":true},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["As we can see above, both models seem good, we will now see how good they are both on train set and test set"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import abs\nprint (\"explained Variance for non null values={}\".format(summary.explainedVariance))\nprint (\"meanAbsoluteError for non null values=%g\" %summary.meanAbsoluteError)\n\nprint (\"explainedVariance for values replaced={}\".format(summary_avg.explainedVariance))\nprint (\"meanAbsoluteError for values replaced=%g\" %summary_avg.meanAbsoluteError)\n\n#For printing the Mean Asbolute error, we could also have wirtten this : \ntrainResults=lrModel.evaluate(trainData)  #evaluation on the training set\ntrainResults_avg=lrModel_avg.evaluate(trainData_avg)  #evaluation on the training set\ndf1= trainResults.residuals #Difference between prediction and reality\ndf1_avg= trainResults_avg.residuals #Difference between prediction and reality \ndf1.select(abs(df1.residuals)).groupBy().avg().show()\ndf1_avg.select(abs(df1_avg.residuals)).groupBy().avg().show() "],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["The dataset with null values seems better. Let's now apply  our model on test set"],"metadata":{}},{"cell_type":"code","source":["testResults = lrModel.evaluate(testData) #Use the model on our test data\ntestResults_avg = lrModel_avg.evaluate(testData_avg) #Use the model on our test data\n\ntestResults.residuals.show(n=10)  #Show difference between prediction and reality\ntestResults_avg.residuals.show(n=10)  #Show difference between prediction and reality"],"metadata":{"collapsed":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":["print (\"r2 for non null values=%g\"%testResults.r2)   # my model explains x % of the variance of the data\nprint (\"r2 for values replaced=%g\"%testResults_avg.r2)   # my model explains x % of the variance of the data\n\nprint (\"rootMeanSquaredError for non null values=%g\"%testResults.rootMeanSquaredError)   # RMSE\nprint (\"rootMeanSquaredError for values replaced=%g\"%testResults_avg.rootMeanSquaredError)   # RMSE\n\nprint (\"meanAbsoluteError for non null values=%g\"%testResults.meanAbsoluteError)\nprint (\"meanAbsoluteError for values replaced=%g\"%testResults_avg.meanAbsoluteError)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["Our R2 score is closer  to 1 with our data with all null values dropped, as is the RMSE lower and the mean asbolute error lower than with null values replaces by the average. So for the following we will use the \"rowData\" dataset and no longer the rowData_avg dataset."],"metadata":{}},{"cell_type":"markdown","source":["## CrossValidation and ParamGridBuilder"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n\nevaluator = RegressionEvaluator(metricName='rmse')\nevaluator.explainParam('metricName')\npipeLine=Pipeline()\npipeLine.setStages([lr])\n\nparamGrid = ParamGridBuilder().addGrid(lr.regParam, [0.1, 0.01]).build()\n\ncrossval = CrossValidator(estimator=pipeLine,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=2)\ncvModel = crossval.fit(trainData)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["cross_prediction = cvModel.transform(testData)\ncross_prediction.show()\n"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["test = cross_prediction.withColumn('Abs_diff', abs(cross_prediction.label - cross_prediction.prediction))\ncross_prediction=test\ncross_prediction.printSchema()\ncross_prediction.select('Abs_diff').groupBy().avg().show()\n\ncvm = crossval.fit(trainData)\ncross_prediction_test = cvm.transform(testData)\nevaluator.evaluate(cross_prediction_test)"],"metadata":{},"outputs":[],"execution_count":27}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","nbconvert_exporter":"python","file_extension":".py"},"name":"LinearRegression","notebookId":628386786981642},"nbformat":4,"nbformat_minor":0}
